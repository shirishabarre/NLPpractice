{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNABeqnxFv7iIJtK4hzXnvk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirishabarre/NLPpractice/blob/main/Extracting_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZl4kujpQor-",
        "outputId": "3d793402-a952-4f34-aaa5-80a651d39a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/232.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from PyPDF2 import PdfFileReader"
      ],
      "metadata": {
        "id": "xSuqjdpfRt4q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PyPDF2.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ScAbxgv3STMS",
        "outputId": "2d92ac10-7e51-4ec2-db2c-c8e2bac039b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.0.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf=open(\"/content/file1pdf.pdf\",\"rb\")\n",
        "pdf_reader=PyPDF2.PdfReader(pdf)\n",
        "print(\"Number of pages:\",len(pdf_reader.pages))\n",
        "page=pdf_reader.pages[1]\n",
        "print(page.extract_text())\n",
        "pdf.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZMWJVvRSZMR",
        "outputId": "44f0e1a8-ffc1-4610-c5d7-619932145b6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages: 35\n",
            " \n",
            " \n",
            " Development  Plan for Greater Mumbai 2014‐2034                                                                                                                                                                                                                                                      \n",
            "Acknowledgements  \n",
            "The Consultant  wishes to thank the following  individuals  from the Municipal  Corporation  of \n",
            "Greater Mumbai for their invaluable  support, insights and contributions  towards ‘Working  Paper 1 \n",
            "– Preparation  of Base Map’ for the preparation  of the Development  Plan for Greater Mumbai \n",
            "2014‐34. \n",
            " Mr. Subodh Kumar, IAS, Municipal  Commissioner;  \n",
            " Mr. Rajeev Kuknoor, Chief Engineer Development  Plan; \n",
            " Mr. Sudhir Ghate, Deputy Chief Engineer Development  Plan; \n",
            " Mr. A.G. Marathe, Deputy Chief Engineer Development  Plan; \n",
            " Mr. R. Balachandran,  Executive  Engineer and Town Planning Officer, Development  Plan. \n",
            " Our gratitude  to the following  experts for their invaluable  insights and support: \n",
            " \n",
            "Mr. V.K Phatak, Former Chief Town Planner (MMRDA);  \n",
            " Mr. A.N Kale, Former Chief Engineer, (DP); \n",
            " Mr. A. S Jain Former Dy. Chief Engineer, (DP). \n",
            " We wish to especially  thank MCGM officers, Mr. Jagdish Talreja, Mr. Dinesh Naik, Mr. Hiren \n",
            "Daftardar,  Ms. Anita Naik for their continual  support since the\n",
            " beginning  of the project and their \n",
            "help towards familiarization  and data collection.  They have been instrumental  in helping to \n",
            "contact various MCGM departments  as well as in helping to establish contact with personnel  from \n",
            "other government  departments  and organizations.  Many thanks for the MCGM team, for \n",
            "deploying  personnel,  particularly  Mr. Prasad Gharat, on extensive  field visits that have helped in \n",
            "understanding  actual ground conditions.  \n",
            " \n",
            "We apologize  if we have inadvertently  omitted anyone to whom acknowledgement  is due. We hope \n",
            "and anticipate  the work's usefulness  for the intended purpose. \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2, urllib, nltk\n",
        "from io import BytesIO\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "cuCi9PoOTJl2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Wc2fKsWDRJ",
        "outputId": "d4b86a91-a1ce-42d1-b759-dd89ac6b1a2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wFile = urllib.request.urlopen('https://www.udri.org/pdf/02%20working%20paper%201.pdf')\n",
        "pdfreader = PyPDF2.PdfReader(BytesIO(wFile.read()))"
      ],
      "metadata": {
        "id": "fWS9sSuWVPn-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4nM-FKYXnCc",
        "outputId": "c44aab88-3b49-4284-aa3c-e9a7dbd98188"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pageObj=pdfreader.pages[2]\n",
        "page2=pageObj.extract_text()\n",
        "punctuations=['(',')',';',':','[',']',',','...','.']\n",
        "tokens=word_tokenize(page2)\n",
        "stop_words=stopwords.words('english')\n",
        "keywords=[word for word in tokens if not word in stop_words and not word in punctuations]\n"
      ],
      "metadata": {
        "id": "j01NCqKlVoEz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8aNiOdwXj66",
        "outputId": "5f75d01b-05cf-4524-ab68-3aa63dbc42c6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2014‐2034',\n",
              " 'Table',\n",
              " 'Contents',\n",
              " 'The',\n",
              " 'Consultant',\n",
              " 'wishes',\n",
              " 'thank',\n",
              " 'following',\n",
              " 'individuals',\n",
              " 'Municipal',\n",
              " 'Corporation',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " 'invaluable',\n",
              " 'support',\n",
              " 'insights',\n",
              " 'contributions',\n",
              " 'towards',\n",
              " '‘',\n",
              " 'Working',\n",
              " 'Paper',\n",
              " '1',\n",
              " '–',\n",
              " 'Preparation',\n",
              " 'Base',\n",
              " 'Map',\n",
              " '’',\n",
              " 'preparation',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2014‐34',\n",
              " '.............................................................................................................................',\n",
              " '..............',\n",
              " '3',\n",
              " 'Our',\n",
              " 'gratitude',\n",
              " 'following',\n",
              " 'experts',\n",
              " 'invaluable',\n",
              " 'insights',\n",
              " 'support',\n",
              " '............................',\n",
              " '3',\n",
              " 'We',\n",
              " 'wish',\n",
              " 'especially',\n",
              " 'thank',\n",
              " 'MCGM',\n",
              " 'officers',\n",
              " 'Mr.',\n",
              " 'Jagdish',\n",
              " 'Talreja',\n",
              " 'Mr.',\n",
              " 'Dinesh',\n",
              " 'Naik',\n",
              " 'Mr.',\n",
              " 'Hiren',\n",
              " 'Daftardar',\n",
              " 'Ms.',\n",
              " 'Anita',\n",
              " 'Naik',\n",
              " 'continual',\n",
              " 'support',\n",
              " 'since',\n",
              " 'beginning',\n",
              " 'project',\n",
              " 'help',\n",
              " 'towards',\n",
              " 'familiarization',\n",
              " 'data',\n",
              " 'collection',\n",
              " 'They',\n",
              " 'instrumental',\n",
              " 'helping',\n",
              " 'contact',\n",
              " 'various',\n",
              " 'MCGM',\n",
              " 'departments',\n",
              " 'well',\n",
              " 'helping',\n",
              " 'establish',\n",
              " 'contact',\n",
              " 'personnel',\n",
              " 'government',\n",
              " 'departments',\n",
              " 'organizations',\n",
              " 'Many',\n",
              " 'thanks',\n",
              " 'MCGM',\n",
              " 'team',\n",
              " 'deploying',\n",
              " 'personnel',\n",
              " 'particularly',\n",
              " 'Mr.',\n",
              " 'Prasad',\n",
              " 'Gharat',\n",
              " 'extensive',\n",
              " 'field',\n",
              " 'visits',\n",
              " 'helped',\n",
              " 'understanding',\n",
              " 'actual',\n",
              " 'ground',\n",
              " 'conditions',\n",
              " '........................................................................................',\n",
              " '3',\n",
              " 'BEST',\n",
              " '...............................................................................................................................',\n",
              " '.................',\n",
              " '5',\n",
              " 'Brihanmumbai',\n",
              " 'Electric',\n",
              " 'Supply',\n",
              " 'Transport',\n",
              " 'Undertaking',\n",
              " '..............................................................',\n",
              " '5',\n",
              " 'CIDCO',\n",
              " '...............................................................................................................................',\n",
              " '..............',\n",
              " '5',\n",
              " 'City',\n",
              " 'Industrial',\n",
              " 'Development',\n",
              " 'Corporation',\n",
              " '...............................................................................',\n",
              " '5',\n",
              " 'CTP',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Comprehensive',\n",
              " 'Transportation',\n",
              " 'Plan',\n",
              " '...............................................................................................',\n",
              " '5',\n",
              " 'DP',\n",
              " '...............................................................................................................................',\n",
              " '....................',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " '..........................................................................................................................',\n",
              " '5',\n",
              " 'DPGM34',\n",
              " '...............................................................................................................................',\n",
              " '..........',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2034',\n",
              " '.......................................................................................',\n",
              " '5',\n",
              " 'DCR',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Control',\n",
              " 'Regulations',\n",
              " '...................................................................................................',\n",
              " '5',\n",
              " 'DGPS',\n",
              " '...........................................................................................................................',\n",
              " '....................',\n",
              " '5',\n",
              " 'Digital',\n",
              " 'Global',\n",
              " 'Positioning',\n",
              " 'System',\n",
              " '...................................................................................................',\n",
              " '5',\n",
              " 'DPGM',\n",
              " '...............................................................................................................................',\n",
              " '..............',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '...........................................................................................',\n",
              " '5',\n",
              " 'ELU',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Existing',\n",
              " 'Land',\n",
              " 'use',\n",
              " '.............................................................................................................................',\n",
              " '5',\n",
              " 'FSI',\n",
              " '...............................................................................................................................',\n",
              " '....................',\n",
              " '5',\n",
              " 'Floor',\n",
              " 'Space',\n",
              " 'Index',\n",
              " '............................................................................................................................',\n",
              " '5',\n",
              " 'GIS',\n",
              " '...............................................................................................................................',\n",
              " '...................',\n",
              " '5']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_list=list()\n",
        "check=['Mr.','Mrs.','Ms.']\n",
        "for idx, token in enumerate(tokens):\n",
        "  if token.startswith(tuple(check)) and idx<(len(tokens)-1):\n",
        "    name=token+tokens[idx+1]+' '+tokens[idx+2]\n",
        "    name_list.append(name)\n",
        "print(name_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2RJSvvXXrTC",
        "outputId": "f178c6cb-ddd2-47be-cbfd-196b174bd754"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mr.Jagdish Talreja', 'Mr.Dinesh Naik', 'Mr.Hiren Daftardar', 'Ms.Anita Naik', 'Mr.Prasad Gharat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wFile.close()"
      ],
      "metadata": {
        "id": "3G6modl-Y4OK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collecting Data from Word Files\n",
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fE2FzwAY_9Y",
        "outputId": "dd3cf5c6-eb22-45ba-d0a6-ed6b2fa59462"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx"
      ],
      "metadata": {
        "id": "0P2QurkLZ8eL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=open(\"/content/AD BT-9 RESEARCH PAPER[1].docx\",\"rb\")\n",
        "document=docx.Document(doc)"
      ],
      "metadata": {
        "id": "AxWlkaitZ-lR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docu=\"\"\n",
        "for para in document.paragraphs:\n",
        "  docu+=para.text\n",
        "print(docu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyXeD7yqaUCt",
        "outputId": "062002c8-6ee6-454f-9272-49b9b04d3dbd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMARTSYNC:INTELLIGENT RESUME SCREENING USING NLP2211CS020046-B.Mithun  2211CS020048-B.Bhavya2211CS020049-B.Sushmitha2211CS020050-B.Ruchitha2211CS020051-B.ShirishaProject Guide: Prof.MD Shafi Department of CSE (AI&ML)               MALLA REDDY UNIVERSITY        HYDERABADAbstract: The goal of resume screening is to find the best candidates for a position. Our system is a resume ranking software, Input would be resumes and job descriptions, output is highly ranked candidate’s resume and acquired instantly in real-time. We will be using Mong for string matching, Cosine Similarity, TF-IDF. The existing systems are simple and effective but are not robust in terms of accuracy, efficiency, and processing and could lead to inaccurate assumptions and loss of human potential. We propose a web application that aims to order the resumes, by intelligently reading job descriptions as input and comparing the resumes which fall into the category of given Job Descriptions. In order to match and rate candidates in real-time, the software provides a ranking after filtering and recommends the better resume for a given textual job description. The Advantages of the proposed system are Secured, Interpretability, High accuracy, Lightweight model & fast processing. It could be used in MNC’s where multiple resumes must be screened every single day for multiple jobs.INTRODUCTION: The evolving technology is creating many chances of employment for many. Nowadays to apply for any job the most essential document is a resume. A resume tells a lot about the person's achievements and the skill sets in all walks of life. The person applying for the job highlights the strong points and skillsets required for the company. Multinational organizations receive thousands of emails from such people who send their resumes for them to apply for a certain post. Now the real challenge is to know which resume is to be sorted and shortlisted according to the constraints. One method is to manually check and sort the resume. Now this method is the most time-consuming and also can lead to a lot of errors because of human interventions. Also humans cannot keep on working continuously. Hence there is a problem of less efficiency as well. Thus we have proposed a system that will easily find the required skill set by scanning the document or the resume and sort according to the skill sets which is a specified constraint of the organization. We are going to use the concept of Machine learning. Machine learning for recruiting is an emerging category of HR technology designed to reduce or even remove time-consuming activities like manually screening resumes.2. EXISTING MODEL: The existing system is a traditional Machine Learning based system. It gives lower rates of accuracy. It has lower efficiency. It gives lower inaccurate results. This system may lead to loss of human potential. The methods used are NER (Named Entity Recognition), section based segmentation. Existing working methods are time consuming and may review the same resume multiple times if not recognized. The amount of manual work involved in recruiting processes has been reduced, and the initial screening of candidates has been completed. By removing the redundant candidates, you can keep only the ones who are relevant.3.PROPOSED MODEL: Our system is a resume ranking software that uses natural language processing (NLP) and machine learning. This AI-powered resume screening program goes beyond keywords to contextually screen resumes. Following resume screening, the software rates prospects in real-time depending on the recruiter's job needs. The web application aims to order the resumes, by intelligently reading job descriptions as input and comparing the resumes which fall into the category of given Job Descriptions. In order to match and rate candidates in real-time, the software employs natural language processing. Unlike generic processes, this app utilizes Mong for string matching, Cosine Similarity, Overlapping coefficient Natural Language. Our work takes a different approach as it focuses mainly on the content of the resumes where we perform the extraction of skills and related parameters to match candidates with the job descriptions.The interactive web application will allow the job applicants to submit their resumes and apply for job postings they may still be interested in. The resumes submitted by the candidates are then compared with the job profile requirement posted by the company recruiter by using techniques like machine learning.                               PROPOSED MODEL4.RESEARCH METHODOLOGY:1.NATURAL LANGUAGE PROCESSING : This application makes use of Natural Language Processing (NLP) which helps in data training and feature extraction of the text data. NLP is an analysis of natural languages so that computers can understand them. Natural language, whether spoken, written, or typed, is the most natural means of communication between humans, and the mode of expression of choice for most of the documents they produce. Using NLP methods, semi-structured text data is converted to a structured format with required extracted features. Resume collection is being performed and folder Structure creation is being done. Data Cleaning such as removing clutter and unnecessary punctuation would be taken off, Feature Engineering would be performed for enhancement. This includes removing stop words, punctuation, and stemming.2.COSINE SIMILARITY: We've chosen the Cosine Similarity Algorithm, in which the employer's Job Description is matched against the content of resumes in the space, and the topmost similar resumes are suggested to the recruiter. Cosine Similarity is a measurement that quantifies the similarity between two or more vectors. The cosine similarity is the cosine of the angle between vectors. The vectors are typically non-zero and are within an inner product space. The cosine similarity is described mathematically as the division between the dot product of vectors and the product of the euclidean norms or magnitude of each vector. Cosine similarity is a commonly used similarity measurement technique that can be found in widely used libraries and tools such as Matlab, SciKit-Learn, TensorFlow etc.3. TF-IDF: At this stage, a dynamic Script for the Tf-Idf approach is written. Term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection TF-IDF is word frequency scores that aim to emphasize phrases that are more interesting, e.g., common in a text but not across texts, without delving into the arithmetic. The TF-IDF Vectorizer tokenizes texts, learns vocabulary, inverts frequency weightings, and allows encoding new ones. It provides information on a word frequency in the documents. Higher the TF- IDF score of a term which is computed using the above equations represents more relevance in a document.4. Latent Dirichlet Allocation: A tool and technique for Topic Modeling, Latent Dirichlet Allocation (LDA) classifies or categorizes the text into a document and the words per topic, these are modeled based on the Dirichlet distributions and processes. Latent Dirichlet Allocation has been used in the application for the following functions-Discovering the hidden themes in the data. Classifying the data into the discovered themes. Using the classification to organize/summarize/search the documents. The application, then deals with the calculation of the score for a candidate’s resume according to the job posting they have applied for.5. CONCLUSION AND FUTURE SCOPE: Our algorithm was successfully able to screen and shortlist the best candidates with the help of NLP.Highly accurate results were obtained by using Latent Dirichlet Allocation to display the best-shortlisted resume on Web Ui. The web application was successfully able to order the resumes, by intelligently reading job descriptions as input and comparing the resumes which fall into the category of given Job Descriptions. The results from the model are encouraging. The resume classifier application is successful in automating the manual task of project allocation to the new recruits of the organization based on the interests, work experience, and expertise mentioned by the candidate in the profile. The application can be extended further to other domains like Telecom, Healthcare, E-commerce, and public sector jobs. We also wish to put into effect and present a smart evaluation in the consistent database to survey with the present models. Efforts can be made to explore whether it is possible to identify in advance what lists might be ranked worse than the current baseline and to investigate whether there is another way to transform word embeddings into document embeddings.6. REFERENCES: [1] Sujit Amin, Nikita Jayakar, Sonia Sunny, Pheba Babu, M. Kiruthika, Ambarish Gurjar, “Web Application for Screening Resume”, International Conference on Nascent Technologies in Engineering (ICNTE), 2020.   [2] Tumula Mani Harsha, Gangaraju Sai Moukthika, Dudipalli Siva Sai, Mannuru Naga Rajeswari Pravallika, Satish Anamalamudi, MuraliKrishna Enduri, “Automated Resume Screener using Natural Language Processing(NLP)”, 6th International Conference on Trends in Electronics and Informatics (ICOEI), 2022.[3] M. Alamelu, D. Sathish Kumar, R. Sanjana, J. Subha Sree, A. Sangeerani Devi, D. Kavitha, “Resume Validation and Filtration using Natural Language Processing”, 10th International Conference on Internet of Everything, Microwave Engineering, Communication and Networks (IEMECON), 2022.[4] Rasika Ransing, Akshaya Mohan, Nikita Bhrugumaharshi Emberi, Kailas Mahavarkar, “Screening and Ranking Resumes using Stacked Model”, 5th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques (ICEECCOT), 2022.[5] M.F. Mridha, Rabeya Basri, Muhammad Mostafa Monowar, Md. Abdul Hamid, “A Machine Learning Approach for Screening Individual’s Job Profile Using Convolutional Neural Network”, International Conference on Science & Contemporary Technologies (ICSCT), 2021.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(document.paragraphs)):\n",
        "  print(\"The content of the paragraph \"+str(i)+\" is : \"+document.paragraphs[i].text+\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtwg6zCJbEYD",
        "outputId": "7be065e7-a1ae-477b-db35-b874af0879c2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The content of the paragraph 0 is : SMARTSYNC:INTELLIGENT RESUME SCREENING \n",
            "\n",
            "The content of the paragraph 1 is : USING NLP\n",
            "\n",
            "The content of the paragraph 2 is : \n",
            "\n",
            "The content of the paragraph 3 is : 2211CS020046-B.Mithun  2211CS020048-B.Bhavya\n",
            "\n",
            "The content of the paragraph 4 is : 2211CS020049-B.Sushmitha\n",
            "\n",
            "The content of the paragraph 5 is : 2211CS020050-B.Ruchitha\n",
            "\n",
            "The content of the paragraph 6 is : 2211CS020051-B.Shirisha\n",
            "\n",
            "The content of the paragraph 7 is : \n",
            "\n",
            "The content of the paragraph 8 is : Project Guide: Prof.MD Shafi Department of CSE (AI&ML)\n",
            "\n",
            "The content of the paragraph 9 is :                MALLA REDDY UNIVERSITY\n",
            "\n",
            "The content of the paragraph 10 is :         HYDERABAD\n",
            "\n",
            "The content of the paragraph 11 is : \n",
            "\n",
            "The content of the paragraph 12 is : \n",
            "\n",
            "The content of the paragraph 13 is : \n",
            "\n",
            "The content of the paragraph 14 is : Abstract: The goal of resume screening is to find the best candidates for a position. Our system is a resume ranking software, Input would be resumes and job descriptions, output is highly ranked candidate’s resume and acquired instantly in real-time. We will be using Mong for string matching, Cosine Similarity, TF-IDF. The existing systems are simple and effective but are not robust in terms of accuracy, efficiency, and processing and could lead to inaccurate assumptions and loss of human potential. We propose a web application that aims to order the resumes, by intelligently reading job descriptions as input and comparing the resumes which fall into the category of given Job Descriptions. In order to match and rate candidates in real-time, the software provides a ranking after filtering and recommends the better resume for a given textual job description. The Advantages of the proposed system are Secured, Interpretability, High accuracy, Lightweight model & fast processing. It could be used in MNC’s where multiple resumes must be screened every single day for multiple jobs.\n",
            "\n",
            "The content of the paragraph 15 is : \n",
            "\n",
            "The content of the paragraph 16 is : INTRODUCTION: The evolving technology is creating many chances of employment for many. Nowadays to apply for any job the most essential document is a resume. A resume tells a lot about the person's achievements and the skill sets in all walks of life. The person applying for the job highlights the strong points and skillsets required for the company. Multinational organizations receive thousands of emails from such people who send their resumes for them to apply for a certain post. Now the real challenge is to know which resume is to be sorted and shortlisted according to the constraints. One method is to manually check and sort the resume. Now this method is the most time-consuming and also can lead to a lot of errors because of human interventions. Also humans cannot keep on working continuously. Hence there is a problem of less efficiency as well. Thus we have proposed a system that will easily find the required skill set by scanning the document or the resume and sort according to the skill sets which is a specified constraint of the organization. We are going to use the concept of Machine learning. Machine learning for recruiting is an emerging category of HR technology designed to reduce or even remove time-consuming activities like manually screening resumes.\n",
            "\n",
            "The content of the paragraph 17 is : \n",
            "\n",
            "The content of the paragraph 18 is : 2. EXISTING MODEL: The existing system is a traditional Machine Learning based system. It gives lower rates of accuracy. It has lower efficiency. It gives lower inaccurate results. This system may lead to loss of human potential. The methods used are NER (Named Entity Recognition), section based segmentation. Existing working methods are time consuming and may review the same resume multiple times if not recognized. The amount of manual work involved in recruiting processes has been reduced, and the initial screening of candidates has been completed. By removing the redundant candidates, you can keep only the ones who are relevant.\n",
            "\n",
            "The content of the paragraph 19 is : 3.PROPOSED MODEL: Our system is a resume ranking software that uses natural language processing (NLP) and machine learning. This AI-powered resume screening program goes beyond keywords to contextually screen resumes. Following resume screening, the software rates prospects in real-time depending on the recruiter's job needs. The web application aims to order the resumes, by intelligently reading job descriptions as input and comparing the resumes which fall into the category of given Job Descriptions. In order to match and rate candidates in real-time, the software employs natural language processing. Unlike generic processes, this app utilizes Mong for string matching, Cosine Similarity, Overlapping coefficient Natural Language. Our work takes a different approach as it focuses mainly on the content of the resumes where we perform the extraction of skills and related parameters to match candidates with the job descriptions.The interactive web application will allow the job applicants to submit their resumes and apply for job postings they may still be interested in. The resumes submitted by the candidates are then compared with the job profile requirement posted by the company recruiter by using techniques like machine learning.\n",
            "\n",
            "The content of the paragraph 20 is :                 \n",
            "\n",
            "The content of the paragraph 21 is :                PROPOSED MODEL\n",
            "\n",
            "The content of the paragraph 22 is : 4.RESEARCH METHODOLOGY:\n",
            "\n",
            "The content of the paragraph 23 is : 1.NATURAL LANGUAGE PROCESSING : This application makes use of Natural Language Processing (NLP) which helps in data training and feature extraction of the text data. NLP is an analysis of natural languages so that computers can understand them. Natural language, whether spoken, written, or typed, is the most natural means of communication between humans, and the mode of expression of choice for most of the documents they produce. Using NLP methods, semi-structured text data is converted to a structured format with required extracted features. Resume collection is being performed and folder Structure creation is being done. Data Cleaning such as removing clutter and unnecessary punctuation would be taken off, Feature Engineering would be performed for enhancement. This includes removing stop words, punctuation, and stemming.\n",
            "\n",
            "The content of the paragraph 24 is : 2.COSINE SIMILARITY: We've chosen the Cosine Similarity Algorithm, in which the employer's Job Description is matched against the content of resumes in the space, and the topmost similar resumes are suggested to the recruiter. Cosine Similarity is a measurement that quantifies the similarity between two or more vectors. The cosine similarity is the cosine of the angle between vectors. The vectors are typically non-zero and are within an inner product space. The cosine similarity is described mathematically as the division between the dot product of vectors and the product of the euclidean norms or magnitude of each vector. Cosine similarity is a commonly used similarity measurement technique that can be found in widely used libraries and tools such as Matlab, SciKit-Learn, TensorFlow etc.\n",
            "\n",
            "The content of the paragraph 25 is : 3. TF-IDF: At this stage, a dynamic Script for the Tf-Idf approach is written. Term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection TF-IDF is word frequency scores that aim to emphasize phrases that are more interesting, e.g., common in a text but not across texts, without delving into the arithmetic. The TF-IDF Vectorizer tokenizes texts, learns vocabulary, inverts frequency weightings, and allows encoding new ones. It provides information on a word frequency in the documents. Higher the TF- IDF score of a term which is computed using the above equations represents more relevance in a document.\n",
            "\n",
            "The content of the paragraph 26 is : \n",
            "\n",
            "The content of the paragraph 27 is : 4. Latent Dirichlet Allocation: A tool and technique for Topic Modeling, Latent Dirichlet Allocation (LDA) classifies or categorizes the text into a document and the words per topic, these are modeled based on the Dirichlet distributions and processes. Latent Dirichlet Allocation has been used in the application for the following functions-Discovering the hidden themes in the data. Classifying the data into the discovered themes. Using the classification to organize/summarize/search the documents. The application, then deals with the calculation of the score for a candidate’s resume according to the job posting they have applied for.\n",
            "\n",
            "The content of the paragraph 28 is : \n",
            "\n",
            "The content of the paragraph 29 is : \n",
            "\n",
            "The content of the paragraph 30 is : \n",
            "\n",
            "The content of the paragraph 31 is : \n",
            "\n",
            "The content of the paragraph 32 is : 5. CONCLUSION AND FUTURE SCOPE: Our algorithm was successfully able to screen and shortlist the best candidates with the help of NLP.Highly accurate results were obtained by using Latent Dirichlet Allocation to display the best-shortlisted resume on Web Ui. The web application was successfully able to order the resumes, by intelligently reading job descriptions as input and comparing the resumes which fall into the category of given Job Descriptions. The results from the model are encouraging. The resume classifier application is successful in automating the manual task of project allocation to the new recruits of the organization based on the interests, work experience, and expertise mentioned by the candidate in the profile. The application can be extended further to other domains like Telecom, Healthcare, E-commerce, and public sector jobs. We also wish to put into effect and present a smart evaluation in the consistent database to survey with the present models. Efforts can be made to explore whether it is possible to identify in advance what lists might be ranked worse than the current baseline and to investigate whether there is another way to transform word embeddings into document embeddings.\n",
            "\n",
            "The content of the paragraph 33 is : \n",
            "\n",
            "The content of the paragraph 34 is : \n",
            "\n",
            "The content of the paragraph 35 is : \n",
            "\n",
            "The content of the paragraph 36 is : \n",
            "\n",
            "The content of the paragraph 37 is : \n",
            "\n",
            "The content of the paragraph 38 is : \n",
            "\n",
            "The content of the paragraph 39 is : \n",
            "\n",
            "The content of the paragraph 40 is : 6. REFERENCES:\n",
            "\n",
            "The content of the paragraph 41 is :  [1] Sujit Amin, Nikita Jayakar, Sonia Sunny, Pheba Babu, M. Kiruthika, Ambarish Gurjar, “Web Application for Screening Resume”, International Conference on Nascent Technologies in Engineering (ICNTE), 2020.   \n",
            "\n",
            "The content of the paragraph 42 is : [2] Tumula Mani Harsha, Gangaraju Sai Moukthika, Dudipalli Siva Sai, Mannuru Naga Rajeswari Pravallika, Satish Anamalamudi, MuraliKrishna Enduri, “Automated Resume Screener using Natural Language Processing(NLP)”, 6th International Conference on Trends in Electronics and Informatics (ICOEI), 2022.\n",
            "\n",
            "The content of the paragraph 43 is : [3] M. Alamelu, D. Sathish Kumar, R. Sanjana, J. Subha Sree, A. Sangeerani Devi, D. Kavitha, “Resume Validation and Filtration using Natural Language Processing”, 10th International Conference on Internet of Everything, Microwave Engineering, Communication and Networks (IEMECON), 2022.\n",
            "\n",
            "The content of the paragraph 44 is : [4] Rasika Ransing, Akshaya Mohan, Nikita Bhrugumaharshi Emberi, Kailas Mahavarkar, “Screening and Ranking Resumes using Stacked Model”, 5th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques (ICEECCOT), 2022.\n",
            "\n",
            "The content of the paragraph 45 is : [5] M.F. Mridha, Rabeya Basri, Muhammad Mostafa Monowar, Md. Abdul Hamid, “A Machine Learning Approach for Screening Individual’s Job Profile Using Convolutional Neural Network”, International Conference on Science & Contemporary Technologies (ICSCT), 2021.\n",
            "\n",
            "The content of the paragraph 46 is : \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ccjHG7FbmpU",
        "outputId": "bc06fa8d-9d1d-450f-b341-53e006ebe49b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.12.2)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request as urllib2\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "_BX5V0g5dA2S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
        "html_doc=response.read()"
      ],
      "metadata": {
        "id": "EGoy9f92dNEt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup=BeautifulSoup(html_doc,'html.parser')\n",
        "strhtm=soup.prettify()\n",
        "print(strhtm[:5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpQA5w_Tdgza",
        "outputId": "0b552365-cee1-40b2-b87c-8da18c67e695"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\" dir=\"ltr\" lang=\"en\">\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <title>\n",
            "   Natural language processing - Wikipedia\n",
            "  </title>\n",
            "  <script>\n",
            "   (function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\\w+$|[^\\w-]+/g,'')+'-clientpref-\\\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\n",
            "\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"caef1de6-3a61-49de-a3ca-015cf8dc9249\",\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Natural_language_processing\",\"wgTitle\":\"Natural language processing\",\"wgCurRevisionId\":1274942014,\"wgRevisionId\":1274942014,\"wgArticleId\":21652,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"All accuracy disputes\",\"Accuracy disputes from December 2013\",\"Harv and Sfn no-target errors\",\"CS1 errors: periodical ignored\",\"CS1 maint: location\",\"Articles with short description\",\"Short description is different from Wikidata\",\"Articles needing additional references from May 2024\",\"All articles needing additional references\",\"All articles with unsourced statements\",\"Articles with unsourced statements from May 2024\",\"Commons category link from Wikidata\",\n",
            "\"Natural language processing\",\"Computational fields of study\",\"Computational linguistics\",\"Speech recognition\"],\"wgPageViewLanguage\":\"en\",\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Natural_language_processing\",\"wgRelevantArticleId\":21652,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgNoticeProject\":\"wikipedia\",\"wgCiteReferencePreviewsActive\":false,\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":0,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":60000,\"wgEditSubmitButtonLabelPublish\":true,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":false,\"wgVector2022LanguageInHeader\":true,\n",
            "\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q30642\",\"wgCheckUserClientHintsHeadersJsApi\":[\"brands\",\"architecture\",\"bitness\",\"fullVersionList\",\"mobile\",\"model\",\"platform\",\"platformVersion\"],\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};RLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"ext.math.styles\":\"ready\",\"skins.vector.search.codex.styles\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.wikimediamessages.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.wikimediaBadges\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"ext.scribunto.logs\",\"site\",\n",
            "\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.js\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.switcher\",\"ext.urlShortener.toolbar\",\"ext.centralauth.centralautologin\",\"mmv.bootstrap\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ea53Fgupdu0-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}